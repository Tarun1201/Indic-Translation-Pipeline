{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from inference.engine import Model as IndicTrans2                       # Translation\n",
    "import warnings\n",
    "import nest_asyncio\n",
    "import asyncio\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing sentencepiece model for SRC and TGT\n",
      "Initializing model for translation\n",
      "WARNING:tensorflow:From f:\\Projects\\translate\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-16 10:37:03 | INFO | fairseq.tasks.translation | [SRC] dictionary: 122706 types\n",
      "2024-02-16 10:37:03 | INFO | fairseq.tasks.translation | [TGT] dictionary: 32296 types\n"
     ]
    }
   ],
   "source": [
    "# Load Model\n",
    "\n",
    "IndicTrans2_model = IndicTrans2(\"models/indic-en-preprint/fairseq_model\", model_type=\"fairseq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read transliteration output file\n",
    "\n",
    "file_path = r\"C:\\Users\\useer\\Downloads\\AP_JSP_YT_Hashtag_Comments_Jan_15_31_2024translit.xlsx\"\n",
    "df = pd.read_excel(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add based on languages needed\n",
    "\n",
    "lang_tag_mapping = {\n",
    "                    'hi': 'hin_Deva',\n",
    "                    'te':'tel_Telu'\n",
    "                    # 'bn': 'ben_Beng'\n",
    "                    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# consol_lang_tag to translation tag mapping\n",
    "\n",
    "filtered_df = df[df['consol_lang_tag'].isin(lang_tag_mapping.keys())] \n",
    "filtered_df['len'] = filtered_df['translit_text'].apply(len)\n",
    "len_filtered_df = filtered_df[filtered_df['len'] < 400]\n",
    "anomaly_df = filtered_df[filtered_df['len'] >= 400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-16 10:41:03 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2024-02-16 10:41:03 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True\n",
      "2024-02-16 10:41:03 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False\n",
      "2024-02-16 10:41:03 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.793877680096134 % completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-16 12:04:24 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2024-02-16 12:04:24 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True\n",
      "2024-02-16 12:04:24 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False\n",
      "2024-02-16 12:04:24 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.0 % completed\n"
     ]
    }
   ],
   "source": [
    "# Language based splitting and batch translating\n",
    "\n",
    "completion_len = 0\n",
    "translated_df = pd.DataFrame()\n",
    "\n",
    "for lang_tag in lang_tag_mapping.keys():\n",
    "    lang_filtered_df = len_filtered_df[len_filtered_df['consol_lang_tag'] == lang_tag]\n",
    "    temp_translated = IndicTrans2_model.batch_translate(lang_filtered_df['translit_text'].to_list(), lang_tag_mapping[lang_tag], \"eng_Latn\")\n",
    "    lang_filtered_df['translated_text'] = temp_translated\n",
    "    translated_df = pd.concat([translated_df, lang_filtered_df], ignore_index=True)\n",
    "    completion_len += len(lang_filtered_df)\n",
    "    print(f'{completion_len * 100 / len(len_filtered_df)} % completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "translated_df = pd.concat([translated_df, anomaly_df], ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_save_path = r'C:\\Users\\useer\\Downloads\\AP_JSP_YT_Hashtag_Comments_Jan_15_31_2024_translated.xlsx'\n",
    "translated_df.to_excel(file_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "translate",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
